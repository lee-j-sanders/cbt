#! /bin/bash
# Specify strip_unit, 4k, 64k or 256k as parameter 1, default is 4K

debug=""
cephcmd="ceph"
rbdcmd="rbd"

stripe_unit=$1
if [ -z "$stripe_unit" ]; then
   stripe_unit=4K
fi 

typeset -i k=4
typeset -i m=2

typeset -i drivesize=210000 #in megabytes
typeset -i fillpc=50
typeset -i numvols=8

# Working out the volume size based on
#(  Number of K drives * drive size ) /numvols 

typeset -i a=$drivesize*$k
typeset -i b=$a*$fillpc/100
typeset -i volsize=$b/$numvols

# need to recalculate the number of PGs with this page if 
# the number of OSDs changes:
# https://docs.ceph.com/en/squid/rados/operations/placement-groups/#placement-groups

echo "volsize = $volsize"

$debug $cephcmd config set mgr mgr/cephadm/use_repo_digest false
sleep 10
$debug $cephcmd orch apply osd --all-available-devices
sleep 120

$debug $cephcmd osd erasure-code-profile set reedsol plugin=lrc k=$k m=$m l=3 stripe_unit=$stripe_unit crush-failure-domain=osd
sleep 10

$debug $cephcmd osd pool create unique_pool_0 64 64 erasure reedsol
sleep 10

$debug $cephcmd osd pool set unique_pool_0 allow_ec_overwrites true
sleep 10
$debug $cephcmd osd pool application enable unique_pool_0 rbd 
sleep 10

$debug $rbdcmd pool init unique_pool_0
sleep 5

# Check PG autoscaler progress, make sure all are in Complete or Not In Progress state
echo "Check PG autoscaler progress, make sure all are in Complete or Not in Progress state"
$debug ceph progress

exit

