#! /bin/bash
# Specify strip_unit, 4k, 64k or 256k as parameter 1, default is 4K

debug=""
cephcmd="ceph"
rbdcmd="rbd"

stripe_unit=$1
if [ -z "$stripe_unit" ]; then
   stripe_unit=4K
fi 

typeset -i k=4
typeset -i m=2

typeset -i drivesize=210000 #in megabytes
typeset -i fillpc=50
typeset -i numvols=8

# Working out the volume size based on
#(  Number of K drives * drive size ) /numvols 

typeset -i a=$drivesize*$k
typeset -i b=$a*$fillpc/100
typeset -i volsize=$b/$numvols

# need to recalculate the number of PGs with this page if 
# the number of OSDs changes:
# https://docs.ceph.com/en/squid/rados/operations/placement-groups/#placement-groups

echo "volsize = $volsize"

$debug $cephcmd config set mgr mgr/cephadm/use_repo_digest false
$debug $cephcmd orch apply osd --all-available-devices
sleep 120

# $debug $cephcmd osd erasure-code-profile set reedsol plugin=clay k=$k m=$m stripe_unit=$stripe_unit crush-failure-domain=osd
$debug $cephcmd osd erasure-code-profile set reedsol k=2 m=1 stripe_unit=$stripe_unit crush-failure-domain=osd

$debug $cephcmd osd pool create unique_pool_0 16 16 erasure reedsol

$debug $cephcmd osd pool set unique_pool_0 allow_ec_overwrites true
$debug $cephcmd osd pool application enable unique_pool_0 rados --yes-i-really-mean-it
# $debug $cephcmd osd set nodeep-scrub
# $debug $cephcmd osd set noscrub
$debug $cephcmd osd primary-affinity 0 0
$debug $cephcmd osd pool set unique_pool_0 fast_read true

$debug $rbdcmd pool init unique_pool_0

# Check PG autoscaler progress, make sure all are in Complete or Not In Progress state
echo "Check PG autoscaler progress, make sure all are in Complete or Not in Progress state"
$debug ceph progress

exit

