#! /bin/bash
# Specify strip_unit, 4k, 64k or 256k as parameter 1, default is 4K

debug=""
cephcmd="ceph"
rbdcmd="rbd"

stripe_unit=$1
if [ -z "$stripe_unit" ]; then
   stripe_unit=256K
fi 

typeset -i k=4
typeset -i m=2

typeset -i drivesize=210000 #in megabytes
typeset -i fillpc=50
typeset -i numvols=1

# Working out the volume size based on
#(  Number of K drives * drive size ) /numvols 

typeset -i a=$drivesize*$k
typeset -i b=$a*$fillpc/100
typeset -i volsize=$b/$numvols

# need to recalculate the number of PGs with this page if 
# the number of OSDs changes:
# https://docs.ceph.com/en/squid/rados/operations/placement-groups/#placement-groups

echo "volsize = $volsize"

$debug $cephcmd config set mgr mgr/cephadm/use_repo_digest false
$debug $cephcmd orch apply osd --all-available-devices

$debug $cephcmd osd erasure-code-profile set reedsol plugin=jerasure k=$k m=$m technique=reed_sol_van stripe_unit=$stripe_unit crush-failure-domain=osd

$debug $cephcmd osd pool create rbd_erasure erasure reedsol
$debug $cephcmd osd pool create rbd_replicated replicated

$debug $cephcmd osd pool application enable rbd_erasure rbd 
$debug $cephcmd osd pool application enable rbd_replicated rbd
$debug $cephcmd osd pool set rbd_erasure allow_ec_overwrites true

$debug $rbdcmd pool init rbd_erasure
$debug $rbdcmd pool init rbd_replicated

hostname=$(hostname -f)
typeset -i endvol=$numvols-1

for i in $(seq 0 $endvol) 
do
   volname="cbt-librbdfio-${hostname}-${i}"
   $debug $rbdcmd create --pool rbd_replicated --data-pool rbd_erasure --size $volsize $volname 
done

# Check PG autoscaler progress, make sure all are in Complete or Not In Progress state
echo "Check PG autoscaler progress, make sure all are in Complete or Not in Progress state"
$debug ceph progress

exit

